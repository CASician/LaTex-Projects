\documentclass[9pt]{extarticle}
\usepackage[a4paper,headsep=10pt, top=30pt, bottom=30pt, left=30pt, right=30pt, footskip=10pt]{geometry}
\usepackage{listings}	
\usepackage{setspace}			%used for \begin{spacing}\end{spacing}-> vertical space setting
\usepackage{amsfonts} 			%used for \mathbb{} -> numerical sets
\usepackage{amsmath}			%used for \text{} inside math equations
\usepackage{enumitem}
\usepackage{bm}					%used for bold math expressions
\usepackage{calc}				%used for algebric operations inside \newenvironment
\usepackage{fancyhdr}			%used for head and foot settings
%\usepackage{draftwatermark}		%used for watermark
\usepackage{clrscode3e}		%pseudocde 
\usepackage{graphics}



%%watermark settings
%\SetWatermarkText{Vici Francesco}
%\SetWatermarkScale{2}
%\SetWatermarkAngle{60}
%%end watermark settings



\fancypagestyle{plain}
{
\fancyhead{}\fancyfoot{}
\fancyhead[C]{\vspace{0pt}   Sommario Algortmi e Strutture Dati}
\fancyfoot[R]{Vici Francesco}
\fancyfoot[C]{\thepage}
}
\pagestyle{plain}



\newenvironment{formulario}
{
\setlength{\columnsep}{3em}
\twocolumn
\lstset{tabsize=3}
\begin{spacing}{1}
\begin{flushleft}
}{
\end{flushleft}
\end{spacing}
}



\newenvironment{tcenter}{
  \par
  \centering
  \setlength{\parskip}{0pt} % Rimuovi spaziatura verticale
  \noindent
}{
  \par
}




\newenvironment{descr}[1]
{
\setlist[description,1]{leftmargin=2em + (2em * #1),labelindent=0em + (2em * #1)}
\begin{description}[topsep=0pt,itemsep=0pt,partopsep=0pt, parsep=0pt]
}{
\end{description}
}



\newenvironment{code}[1]
{
\begin{codebox}
\Procname{$#1$}
}{
\end{codebox}
}



\newenvironment{myParagraph}[1]
{
\begin{tcenter}
\textbf{#1}
\end{tcenter}
}{
\myRule
}



\newcommand{\myRule}{\rule{250pt}{0.1pt}}
\newcommand{\bo}[1]{\textbf{#1}}
\newcommand{\co}[1]{\textit{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\la}{\leftarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\sse}{\leftrightarrow}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\FOR}[1]{\For $#1$ \Do}
\newcommand{\WHILE}[1]{\While $#1$ \Do}
\newcommand{\IF}[1]{\If $#1$ \Then}




\begin{document}
	\begin{formulario}

%PARAGRAFO 1
		\begin{tcenter}
\textbf{ALGORITMO}
		\end{tcenter}
Un algoritmo è una sequenza finita di operazioni, anche dette istruzioni, che riceve un input e restituisce un output al fine di risolvere un determinato problema.\\
\myRule

%PARAGRAFO 2
		\begin{tcenter}
\textbf{PROBLEMA DELL'ORDINAMENTO} 
		\end{tcenter}
Data in input una sequenza di numeri $n$ numeri $<a_1,a_2,...,a_n>$ si vuole ottenere in output una permutazione tale che:
		\begin{tcenter}
$<a_{1}^{'},a_{2}^{'},...,a_{n}^{'}>$ con $a_{1}^{'} \leq a_{2}^{'} \leq ... \leq a_{n}^{'} $
		\end{tcenter}
Il tipo di algoritmo usato per l'ordinamento dipende da fattori come: numero di elementi, ordinamento iniziale, tipo di memorizzazione usata, ecc...\\
\myRule

%PARAGRAFO 3
		\begin{tcenter}
\textbf{CORRETTEZZA DI UN ALGORITMO}
		\end{tcenter}
Un algoritmo viene detto "corretto" se ad ogni istanza di input termina con l'output giusto.\\
\textbf{Induzione Matematica}: Supponendo di voler dimostrare che la proprietà $P(n)$ vale per qualsiasi $n\in \mathbb{N}$. Definisco l'insieme universo $U={n\in\mathbb{N}\text{ t.c. vale } P(n)}$. Allora:\\

	\begin{descr}{1}
		\item[Passo Base]$\rightarrow$ dimostro che vale $P(1)$ o $P(0)$; 
		\item[Passo Induttivo]$\rightarrow$ suppongo che $P(n)$ valga per un generico $n$. Posso dunque concludere che $U$ coincide con $\mathbb{N}$.
	\end{descr}
\textbf{Invariante di Ciclo}: formulo un'affermazione che deve essere verificata in 3 diversi momenti:\\
	\begin{descr}{1}
		\item[Inizializzazione]$\rightarrow$ l'I.C. deve essere vera prima della prima iterazione del ciclo;
		\item[Conservazione]$\rightarrow$ l'I.C. deve essere vera prima della successiva iterazione del ciclo;
		\item[Conclusione]$\rightarrow$ l'I.C. al termine del ciclo deve fornire una condizione che permetta di verificare se l'output dell'algoritmo è corretto.
	\end{descr}
\myRule

%PARAGRAFO 4
	\begin{tcenter}
\textbf{ANALISI ASINTOTICA}
	\end{tcenter}
Il calcolo asintotico è usato per analizzare la complessità di un algoritmo ovvero per stimare quanto tale complessità aumenta all’aumentare della dimensione dell’input. Uso la funzione $T(n)$ in quanto la risorsa considerata è il tempo (misurato in operazioni elementari). Allora:
	\begin{descr}{0}
	
\item[Notazione $\bm{O}$] $\rightarrow$ $g(n)$ è un limite asintotico superiore per $f(n)$ ed è definito come:
		\begin{tcenter}
$\bm{O(g(n))} =\{ f(n): \; \exists \, c\in\R^+ ,n_0\in\N $ t.c. $ \bm{0\leq f(n)\leq c\cdot g(n)} \;\; \forall\, n\geq n_0 \}$
		\end{tcenter} 
		
\item[Notazione $\bm{o}$] $\rightarrow$ è definito come:
		\begin{tcenter}
$\bm{o(g(n))} =\{ f(n): \; \forall \, c\in\R ,\,\exists n_0>0\in\N $ t.c. $ \bm{0\leq f(n)< c\cdot g(n)} \;\; \forall\, n\geq n_0 \}$
		\end{tcenter} 
		
\item[Notazione $\bm{\Omega}$] $\rightarrow$ $g(n)$ è un limite asintotico inferiore per $f(n)$ ed è definito come:
		\begin{tcenter}
$\bm{\Omega(g(n))} =\{ f(n): \; \exists \, c\in\R^+ ,n_0\in\N $ t.c. $ \bm{0\leq c\cdot g(n)\leq f(n)} \;\; \forall\, n\geq n_0 \}$
		\end{tcenter} 		
		
\item[Notazione $\bm{\omega}$] $\rightarrow$ è definito come:
		\begin{tcenter}
$\bm{\omega(g(n))} =\{ f(n): \; \forall \, c\in\R ,\,\exists n_0>0\in\N $ t.c. $ \bm{0\leq c\cdot g(n)< f(n)} \;\; \forall\, n\geq n_0 \}$
		\end{tcenter}

\item[Notazione $\bm{\Theta}$] $\rightarrow$ $g(n)$ è un limite asintoticamente stretto per $f(n)$ ed è definito come:
		\begin{tcenter}
$\bm{\Theta(g(n))} =\{ f(n): \; \exists \, c_1,c_2\in\R^+ ,n_0\in\N $ t.c. $ \bm{0\leq c_1\cdot g(n)\leq f(n)\leq c_2\cdot g(n)} \;\; \forall\, n\geq n_0 \}$
		\end{tcenter} 
	\end{descr}
	
	\begin{descr}{1}
		\item[Teorema]: $f(n)=\Theta g(n) \text{ SSE } f(n)=O g(n) \text{ e } f(n)=\Omega g(n)$
		\item[Proprietà]: 
		\begin{descr}{0}
			\item[Transitiva]: $f(n)=\Theta(g(n))\text{ e }g(n)=\Theta(h(n)) \implies f(n)=\Theta(h(n))$ \quad (vale anche per $O,o,\Omega,\omega$);
			\item[Riflessiva]: $f(n)=\Theta(f(n))$ \quad (vale anche per $O,\Omega$);
			\item[Simmetria]: $f(n)=\Theta(g(n))\Leftrightarrow g(n)=\Theta(f(n))$;
			\item[Simmetria Trasposta]: $f(n)=O(g(n))\Leftrightarrow g(n)=\Omega(f(n))$ $f(n)=o(g(n))\Leftrightarrow g(n)=\omega(f(n))$
		\end{descr}
	\end{descr}
\myRule
%PARAGRAFO 5
	\begin{tcenter}
\textbf{CLASSI DI COMPLESSITA'}
	\end{tcenter}
Per categorizzare la complessità degli algoritmi faccio riferimento alla crescita di funzioni semplici. Le classi sono:\\
	\begin{descr}{0}
	\item[$\bm{O(1)}$] $\rightarrow$ complessità costante;
	\item[$\bm{O(kn)}$] con $k<1 \rightarrow$ complessità sottolineare (\underline{ES}: ricerca sequenziale);
	\item[$\bm{O(n)}$] $\rightarrow$ complessità lineare (\underline{ES}:ricerca sequenziale);
	\item[$\bm{O(n\cdot \ln n)}$] $\rightarrow$ (\underline{ES}: algoritmi di ordinamento ottimi);
	\item[$\bm{O(n^k)}$] con $k\geq 2\rightarrow$ (\underline{ES}: BubbleSort con $O(n^2)$); 
	\item[$\bm{O(k^n)}$] $\rightarrow$ complessità esponenziale;
	\end{descr}
\myRule

%PARAGRAFO 6
	\begin{tcenter}
\textbf{ALGORITMI ITERATIVI - INSERTION SORT}
	\end{tcenter}
È uno degli algoritmi iterativi più semplici. Ordina "in place" ed è efficiente per insiemi quasi ordinati. Lo pseudocodice è:
		\begin{codebox}
\Procname{$\proc{Insertion-Sort}(A)$}
\li \For $j \la 2$ \To $\attrib{A}{length}$ \Do
	\li $\id{key} \la A[j]$
	\li $i \la j-1$
	\li \While $i > 0$ \const{and} $A[i] > \id{key}$ \Do
		\li $A[i+1] \la A[i]$
		\li $i \la i-1$
	\End
	\li $A[i+1] \la \id{key}$
\End
		\end{codebox}
\textbf{Correttezza con I.C.} \\
"All'inizio di ogni iterazione del ciclo \textit{for}, il sottoarray $A[1,\dots,j-1]$ è ordinato ed è formato dagli stessi elementi che erano originariamente in $A[1,\dots,j-1]$, ma ordinati".\\
\textbf{Costo}
		\begin{descr}{1}
\item[Caso Ottimo] (Array Ordinato): $O(n)\rightarrow$ non si entra mai nel \textit{while} ma si esegue ogni volta il confronto, al contrario si esegue sempre il \textit{for}.
\item[Caso Peggiore] (Array Ordinato al Contrario): $O(n^2)\rightarrow$ si esegue il \textit{while} per il numero massimo di volte, il costo del \textit{for} è irrilevante rispetto a quello del \textit{while}.
\item[Caso Medio] (Array Disordinato): $O(n^2)\rightarrow$ il costo dipende dall'ordine ma statisticamente il \textit{while} verrà eseguito soltanto la metà delle volte rispetto al caso peggiore, il costo rimane quadratico. 
		\end{descr}
\myRule

%PARAGRAFO 7
		\begin{tcenter}
\textbf{ALGORITMI ITERATIVI - SELECTION SORT}
		\end{tcenter}
È un algoritmo molto semplice, opera "in place". Il suo obbiettivo è quello di trovare il minimo all'interno dell'array e inserirlo nella sequenza ordinata fino a qual momento. Lo pseudocodice è: \\		
		\begin{code}{SelectionSort(A)}
\li $n\la\attrib{A}{lenght}$
\li \For $j\la1 \To n-1$ \Do
	\li $\id{smallest}\la j$
	\li \For $i \la j+1 \To n$ \Do
		\li \If $A[i]<A[\id{smallest}]$ \Then
			\li $\id{smallest}\la i$
		\End
	\End
	\li \bf{exchange }$A[j]\sse A[smallest]$
		\end{code}				
\textbf{Correttezza con I.C.}
		\begin{descr}{1}
\item[Ciclo Interno] $\rightarrow$ "All'inizio della $i$-esima iterazione del \textit{for} interno, $A[min]$ è minore o uguale di ogni elemento di $A[j,\dots,i-1]$ cioè $\forall j\in [j,\dots,i-1]$ si ha che $A[min]\leq A[j]$". 
\item[Ciclo Esterno] $\rightarrow$ "All'inizio di ogni iterazione del ciclo \textit{for} esterno, il sottoarray $A[1,\dots,j-1]$ è ordinatoe composto solo dagli elementi più piccoli dell'array $A$".
		\end{descr}
\bo{Costo}\\
In questo caso è sempre $O(n^2)$ perché, sia nel caso migliore sia nel caso peggiore, i due cicli \co{for} vengono comunque eseguiti. L'unica riga che può non essere eseguita è la 6, che è trascurabile al fine del calcolo del costo.\\
\myRule

%PARAGRAFO 8
		\begin{tcenter}
\bo{COMPLESSITÀ ALGORITMI RICORSIVI}
		\end{tcenter}
\bo{Equazione di Ricorrenza}\\
Si usa per stimare il tempo di esecuzione di algoritmi ricorsivi. Si fa rifermento alla divisione e combinazione di vari sottoproblemi. Infatti l'equazione è:
		\begin{tcenter}
$\mathbf{T(n)=		
		\begin{cases}
\Theta(1) \qquad \text{se } n\leq c\\
aT(\frac{n}{b})+D(n)+C(n) \qquad \text{altrimenti}
		\end{cases}}$
		\end{tcenter}  
Se $n\leq c$ con $c$ una costante, si ha il caso base; altrimenti si divide in $a$ sottoproblemi di dimensione $\frac{1}{b}$. Questo implica un costo $D(n)$ di divisione del problema e un costo $C(n)$ di combinazione dei sottoproblemi. I metodi di risoluzione di tale equazione sono:
		\begin{descr}{1} 
\item[Metodo di Sostituzione] $\rightarrow$ Ipotizzo una soluzione e la dimostro con un'induzione matematica:\\
			\begin{descr}{0}
\item[1.] indovino una soluzione e formulo un'ipotesi induttiva;
\item[2.] sostituisco nell'equazione di ricorrenza le epressioni $T(\cdot)$ ;
\item[3.] dimostro che è valida anche per il caso base.
			\end{descr}
La soluzione per una ricorrenza può essere:
			\begin{descr}{0}
\item[Esatta] $\rightarrow$ se la ricorrenza è formata da una funzione esatta allora anche la soluzione sarà sempre esatta;
\item[Asintotica] $\rightarrow$ mi riconduco ad una soluzione esatta, ipotizzo per prima cosa che la soluzione $T(n)$ sia per esempio un $O$ di qualche funzione e poi cerco le costanti che verificano l'ipotesi induttiva (quelle presenti nella definizione di $O$). 
			\end{descr}
\item[Metodo dell'Albero di Ricorsione] $\rightarrow$ È un metodo grafico per arrivare alla soluzione. L'albero deve sempre condurre ad un'equazione esatta. I nodi rappresentano i costi dei vari sottoproblemi e permette di aver un'idea del costo complessivo di tutte le esecuzioni dell'algoritmo. Si sommano i costi di tutti i vari sottolivelli per poi fare una somma complessiva di tali sottolivelli.
\item[Metodo dell'Esperto] $\rightarrow$ Sia $T(n)$ una funzione definita sui naturali dalla ricorrenza $T(n)=aT(\frac{n}{b})+f(n)$ con $a\geq 1, b>1$ costanti e $f(n)>0$ asintoticamente, allora $T(n)$ può essere limitata nei seguenti modi:
			\begin{descr}{0}
\item[caso 1]$\rightarrow$ se $f(n)=O(n^{\log_b a-\epsilon})$ per qualche costante $\epsilon>0$ allora: 
				\begin{tcenter}
$\mathbf{T(n)=\Theta(n^{\log_b a})}$
				\end{tcenter} 
\item[caso 2]$\rightarrow$ se $f(n)=\Theta(n^{\log_b a})$ allora:
				\begin{tcenter}
$\mathbf{T(n)=\Theta(n^{\log_b a}\cdot \log_2 n)}$
				\end{tcenter}
\item[caso 3]$\rightarrow$ se $f(n)=\Omega(n^{log_b a+\epsilon})$ per qualche costante $\epsilon>0$ e $f(n)$ t.c. $a\;f(\frac{n}{b})\leq c\;f(n)$ per qualche costante $c<1$ e $\forall n\geq n_0$, allora:
				\begin{tcenter}
$\mathbf{T(n)=\Theta(f(n)}$
				\end{tcenter}
\item[Condizione di Regolarità:]
Devo assicurarmi che quando scendo nell'albero $f(\cdot)$ diventa più piccola.
			\end{descr}
\item[Albero di Ricorsione + Esperto]$\rightarrow$ Combinando i metodi precedentemente descritti ottengo:
			\begin{descr}{0}
\item[caso 1]$\rightarrow$ se il costo cresce dalla radice alle foglie ("Costo Dominato dalle Foglie"), allora:
				\begin{tcenter}
$\mathbf{T(n)=\Theta(n^{\log_b a})}$
				\end{tcenter}
\item[caso 2]$\rightarrow$ se il costo è circa lo stesso in ogni livello, allora:
				\begin{tcenter}
$\mathbf{T(n)=\Theta(n^{\log_b a}\cdot\log_2 n)}$
				\end{tcenter}
\item[caso 3]$\rightarrow$ se il costo decrementa dalla radice alle foglie ("Costo Dominato dalla Radice"), allora: 
				\begin{tcenter}
$\mathbf{T(n)=\Theta(f(n))}$
				\end{tcenter}
			\end{descr}
		\end{descr}
\myRule

%PARAGRAFO 9
		\begin{tcenter}
\bo{ALGORITMI RICORSIVI - MERGE SORT}
		\end{tcenter}
Divide ricorsivamente l'array da ordinare in due sottoarray di uguale lunghezza. Una volta arrivato alla lunghezza unitaria combina ricorsivamente i sottoarray ordinandoli. Lo pseudocodice è:
		\begin{code}{MergeSort(A,p,r)}
\li \IF{p<r} 
	\li $q\la \floor{(p+r)/2}$ 
	\li \proc{MergeSort}(A,p,q)
	\li \proc{MergeSort}(A,q+1,r)
	\li \proc{Merge}(A,p,q,r)
\End
		\end{code}		
		\begin{code}{Merge(A,p,q,r)}
\li $n_1 \la q-p+1$
\li $n_2 \la r-q$
\li \FOR{i \la 1 \To n-1} 
	\li $L[i] \la A[p+i-1]$
\End
\li \FOR{j \la 1 \To n_2} 
	\li $R[i] \la A[q+j]$
\End
\li $L[n_1+1]\la \infty$
\li $R[n_2+1]\la \infty$
\li $i \la 1$
\li $j \la 1$
\li \FOR{k \la \To r}
	\li \IF{L[i]\leq R[j]}
		\li $A[k]\la L[i]$
		\li $i \la i+1$
	\Else
		\li $A[k]\la R[j]$
		\li $j \la j+1$
	\End
\End
		\end{code}		
\bo{Costo}\\
Ho che il costo di divisione è $D(n)=\Theta(1)$ mentre il costo di combinazione è $C(n)=\Theta(n)$ Inoltre ad ogni ricorrenza successiva l'algoritmo risolve $2$ problemi di dimensione $\frac{n}{2}$. Allora l'equazione di ricorrenza sarà: 
		\begin{tcenter}
$
T(n)=
		\begin{cases}
\Theta(1) \qquad\text{se }n=1 \\
2T(\frac{n}{2})+\Theta(1)+\Theta(n) \qquad\text{se }n>1
		\end{cases}
$
		\end{tcenter}
Poiché divido sempre il problema in 2 sottoproblemi di uguale dimensione è facile risolvere il problema con il metodo dell'albero di ricorrenza. Ogni livello ha costo $\Theta(n)$ ed essendo il numero di livelli $\log_2 n$ avrò che il costo del problema è:
		\begin{tcenter} 
$\mathbf{T(n)=}\Theta(n\cdot \log_2 n) + \Theta(n) + \Theta(1)=\mathbf{\Theta(n\cdot \log_2 n)}$
		\end{tcenter}
\myRule

%PARAGRAFO 10
		\begin{tcenter}
\bf{ALGORITMI RICORSIVI - QUICKSORT}
		\end{tcenter}
Divide ricorsivamente l'array da ordinare in due sottoarray grazie all'uso di due indici (ordina sul posto) grazie all'algoritmo "partition" per poi riordinarli e combinarli grazie all'algoritmo "quicksort". Lo pseudocodice è:

		\begin{code}{QuickSort(A,p,r)}
\li \IF{p<r}
	\li $q \la \proc{Partition}(A,p,r)$
	\li $\proc{QuickSort}(A,p,q-1)$
	\li $\proc{QuickSort}(A,q+1,r)$
\End
		\end{code}
		\begin{code}{Partition(A,p,r)}
\li $x\la A[r]$
\li $i\la p-1$
\li \FOR{j\la p \To r-1}
	\li \IF{A[j]\leq x}
		\li $i \la i+1$
		\li	 $\proc{Exchange }A[i] \sse A[j]$
	\End
\End
\li $\proc{Exchange } A[i+1]\sse A[r]$
\li \Return $i+1$	
		\end{code}
\bo{Costo}\\
		\begin{descr}{1}
\item[Partition] $\rightarrow$ ha costo fisso $\Theta(n)$.
\item[Quicksort] $\rightarrow$ il costo dipende dal partizionamento dei sottoarray. Infatti:
			\begin{descr}{0}
\item[Caso migliore] (sottoarray bilanciati) $\rightarrow$ ogni sottoarray ha dimensione $\leq \frac{n}{2}$ quindi il costo sarà $\Theta(\log_2 n)$;
\item[Caso peggiore] (sottoarray sbilanciati) $\rightarrow$ i due sottoarray sono di dimensione $0$ e $n-1$ quindi il costo complessivo sarà $\Theta(n^2)$;
\item[Caso Medio] $\rightarrow$ supponendo una suddivisione parzialmente sbilanciata (ad esempio 9 a 1) avremo un costo:
				\begin{tcenter}
$T(n)=T(\frac{9n}{10})+T(\frac{n}{10})+\Theta(n)=O(n\cdot \log_2 n)$
				\end{tcenter}
			\end{descr}
		\end{descr}
\myRule

%PARAGRAFO 11
	\begin{tcenter}
\bo{CENNI DI PROBABILITÀ}
	\end{tcenter}
Definisco $S$ come lo spazio degli eventi, ovvero come un insieme di eventi $E$ (esiti di esperimenti). Allora la probabilità $P$ che un evento si verifichi è:  
		\begin{tcenter}
		$\bm{P(E)=\frac{\text{casi favorevoli}}{\text{casi possibili}}}$
		\end{tcenter}
\bo{Proprietà:} 
	\begin{descr}{1}
\item[1.] $P(E)\geq 0 \quad \forall \text{ evento }E$;
\item[2.] $P(S)=1$;
\item[3.] $P(A \cup B)=P(A)+P(B)$ \quad se $A$ e $B$ sono mutuamente esclusivi;
\item[3.] $P(A \cup B)=P(A)+P(B)-P(A\cap B)$ \quad se $A$ e $B$ sono compatibili (cioè hanno eventi in comune).
	\end{descr}
\bo{Probabilità Composta:}
	\begin{descr}{1}
	
\item[Eventi Indipendenti]$\rightarrow$ Due eventi sono indipendenti se il realizzarsi di uno non influenza la probabilità di realizzarsi dell'altro. La probabilità che si realizzino entrambi è:
		\begin{tcenter}
$\bm{P(A\cap B)=P(A)\cdot P(B)}$
		\end{tcenter}
		
\item[Eventi Dipendenti]$\rightarrow$ Due eventi sono dipendenti se il realizzarsi di uno influenza la probabilità di realizzarsi dell'altro.  Indico con $P(B|A)$ la probabilità che si verifichi $B$ supponendo che si sia verificato $A$, allora la probabilità che si realizzino entrambi è:
		\begin{tcenter}
$\bm{P(A\cap B)= P(A)\cdot P(B|A)}$
		\end{tcenter}
		
	\end{descr}
\bo{Probabilità Condizionata:}
		\begin{tcenter}
$\bm{P(A|B)=\frac{P(A\cap B)}{P(B)}}$
		\end{tcenter}
		
		\begin{descr}{1}
\item[Teorema di Bayes] $\rightarrow$ Poiché $A\cap B=B\cap A$ ho che:
			\begin{tcenter}
$\bm{P(A\cap B)=P(B)P(A|B)=P(A)P(A|B)}$
			\end{tcenter}
		\end{descr}

	\begin{descr}{0}
\item[Variabile Aleatoria] $\rightarrow$ È l'insieme di tutti i possibili risultati che possono verificarsi in un esperimento.
\item[Distribuzione di Probabilità] $\rightarrow$ Elenca le probabilità di verificarsi di ogni valore della variabile aleatoria a cui si riferisce. La somma di tutte le probabilità di una distribuzione di probabilità è sempre pari a 1 (cioè $\sum P(X)=\sum_i P(x_i)=1$).
\item[Valore Atteso] $\rightarrow$ È il valore medio che mi aspetto da una lunga serie di osservazioni, ed è definito come: 
		\begin{tcenter}
$\bm{\mu=E[X]=\sum_{i=1}^n x_i\cdot P(x_i)}$
		\end{tcenter}
		\begin{descr}{0}
\item[Linearità]:
		\begin{tcenter}
$\bm{E[X+Y]=E[X]+E[Y]}$;\\
$g(x)=ax \implies \bm{E[aX]=aE[X]}$.
		\end{tcenter}
		\end{descr}
	\end{descr}
\myRule

%PARAGRAFO 12
		\begin{tcenter}
\bf{VARIABILI CASUALI INDICATRICI}
		\end{tcenter}
Dato uno spazio dei campioni $S$ e un evento $A$, si definisce variabile casuale indicatrice:
		\begin{tcenter}
$\bm{I\{A\}\text{ oppure }X_A=}		
		\begin{cases} 
1 \quad\text{se si verifica A;}\\
0 \quad\text{se non si verifica A}  
		\end{cases}$
		\end{tcenter}
		\begin{descr}{0}
\item[Lemma] $\rightarrow$ $E[X_a]=Pr\{ A \}$.
		\end{descr}
\myRule

%PARAGRAFO 13
		\begin{tcenter}
\bf{ALGORITMI RANDOMIZZATI}
		\end{tcenter}
Poiché non posso conoscere l'ordine in cui l'algoritmo riceverà gli input, non posso, allo stesso modo, conoscere la probabilità delle $n!$ possibili permutazioni. Uso l'algoritmo "random" per forzare l'ordine casuale. Lo pseudocodice è:
		\begin{code}{RandomizeInPlace(A)}
\li $n \la A.lenght$
\li \FOR{i \la 1 \To n}
	\li $\proc{excange } A[i]\sse A[\proc{Random}(i,n)]$
\End
		\end{code}
\bo{Costo:}\\
$\Theta(1)$ per ogni iterazione, quindi $\Theta(n)$ in totale.
\myRule

%PARAGRAFO 14
		\begin{tcenter}
\bo{ALGORITMI RANDOMIZZATI - QUICKSORT} 
		\end{tcenter}
Cerco di avvicinarmi il più possibile all'ipotesi di caso medio. Per farlo, randomizzo la scelta del pivot $q$ per l'algoritmo "partition", mentre l'algoritmo "quicksort" rimane uguale. Lo pseudocodice è: 
		\begin{code}{RandomizedPartition(A,p,r)}
\li $i\la \proc{Random}(p,r)$
\li \proc{Exchange }$A[r]\sse A[i]$
\li \Return $\proc{Partition} (A,p,r)$

		\end{code}
		\begin{code}{RandomizedQuickSort(A,p,r)}
\li \IF{p<r}
	\li $q\la$ \proc{RandomizedPartition}(A,p,r)
	\li \proc{RandomizedQuickSort}(A,p,q-1)
	\li \proc{RandomizedQuickSort}(A,q+1,r)
\End
		\end{code}
\bo{Costo}\\
Il tempo di esecuzione atteso di RandomizedQuickSort è $O(n\log_2n)$ in quanto statisticamente rispecchia il caso medio di QuickSort. 
\myRule

%PARAGRAFO 15
		\begin{tcenter}
\bo{ALBERO DI DECISIONE}
		\end{tcenter}
È un albero binario che rappresenta i possibili confronti fatti da un algoritmo su un input di una specifica dimensione. La radice rappresenta le prima due posizioni confrontate. Si procede a cascata analizzando tutti i possibili casi. Le foglie rappresenteranno infine tutte le possibili combinazioni degli input. 
		\begin{descr}{1}
\item[\underline{NOTA BENE}] La radice e i nodi intermedi sono strutturati come ("a":"b"). I confronti si effettuano sulla possibilità che sulla posizione "a" vi sia un valore $\leq />$ di quello in posizione "b". Non si fa dunque riferimento allo specifico valore ma solo alla relazione che lega la coppia di posizioni.
		\end{descr}
		\begin{descr}{0}
\item[Lemma] $\rightarrow$ Ogni albero binario di altezza $h$ ha al massimo $2^h$ foglie.
\item[Teorema] $\rightarrow$ Ogni albero di decisione che ordina $n$ elementi ha altezza $\Omega(n\log_2n)$ (cioè nel "caso migliore").
		\end{descr}
\myRule

%PARAGRAFO 16
		\begin{tcenter}
\bo{ORDINAMENTO IN TEMPO LINEARE}
		\end{tcenter}
Gli algoritmi visti fino ad ora non raggiungono un costo minore di $\Theta(n\log_2n)$, è possibile fare di meglio? In ogni caso si ha un costo di $\Omega(n)$ per esaminare tutti gli input.
\myRule

%PARAGRAFO 17
		\begin{tcenter}
\bo{ALGORITMI ITERATIVI - COUNTING SORT}
		\end{tcenter}
È un algoritmo di ordinamento che non si basa sui confronti, ma richiede due array di supporto per effettuare l'ordinamento. Inoltre può ordinare solo numeri naturali. Lo pseudocodice è: 
		\begin{code}{CountingSort(A,B,k)}
\li \FOR{i\la 0\To k}
	\li $C[i]\la 0$
\End
\li \FOR{j\la 1 \To \attrib{A}{lenght}}
	\li $C[A[j]]\la C[A[j]]+1$
\End
\li \FOR{i\la 1\To k}
	\li $C[i]\la C[i]+C[i-1]$
\End
\li \FOR{j\la \attrib{A}{lenght}\Downto 1}
	\li $B[C[A[j]]]\la A[j]$
	\li $C[A[j]]\la C[A[j]]-1$
\End
		\end{code}
\bo{Costo}\\
Il costo di CountingSort è $\Theta(n+k)$, che diventa $\Theta(n)$ se $k=O(n)$. Imponendo la dimensione massima di $k$ come condizione iniziale ottengo un ordinamento in tempo lineare.
		\begin{descr}{1}
\item[\underline{NOTA BENE}] CountingSort è stabile (se due valori sono uguali, il primo nell'array in input rimane primo nell'array di output).
		\end{descr} 
\myRule 

%PARAGRAFO 18
		\begin{tcenter}
\bo{ALGORITMI ITERATIVI - RADIX SORT}
		\end{tcenter}
È un algoritmo di ordinamento che usa il concetto controintuitivo di ordinare le singole cifre a partire dalla meno significativa. Necessita inoltre di riceve in ingresso in numero di cifre "d". Lo pseudocodice è: 
		\begin{code}{RadixSort(A,d)}
\li \FOR{i \la 1 \To d}
	\li "usa ordinamento stabile per ordinare l'array $A$ sulla cifra $i$"
\End
		\end{code}
\bo{Correttezza con I.C.}\\
"Prima della $i$-esima iterazione i numeri sono ordinati sulla base della $i-1$esima cifra meno significativa". \\
\bo{Costo}\\
Supponendo di usare CountingSort come algoritmo di ordinamento stabile, avrò che il costo complessivo è $\Theta(d(n+k))$ ed utilizzando la condizione $k=O(n)$ diventa $\Theta(d\cdot n)$. \\
\bo{Bilanciare Parole e Cifre} \\
Supponendo di dover ordinare $n$ parole di $b$ bit divise in cifre da $r$ bit, ho che il costo del CountingSort sarà $\bm{\Theta(\frac{b}{r}(n+2^r))}$. In particolare scegliendo $r\approx\log_2n$ ho che il costo (sostituendo) diventa $\bm{\Theta(\frac{b\cdot n}{\log_2n})}$ (meglio di $\Theta(n)$).
\myRule

%PARAGRAFO 19
		\begin{myParagraph}{HASHING}
Per la realizzazione di dizionari efficienti si usano le tabelle Hash che hanno tempo di ricerca atteso $O(1)$ mentre hanno $\Theta(n)$ nel caso peggiore. Dato un universo delle chiavi $U=\{ 0,1,\dots,m-1 \}$ esistono diversi modi per implementarle, ovvero:
			\begin{descr}{0}
				\item[Indirizzamento Diretto] $\rightarrow$ spesso il numero di chiavi memorizzate $K$ è molto minore dell'insieme delle chiavi $U$, si spreca quindi molto spazio nella tabella. Per risolvere questo problema si usano le funzioni Hash $h(k)$ memorizzando il valore in $T[h(k)]$ e non in $T[k]$. 
					\begin{descr}{0}
						\item[Metodo delle Divisioni] $\rightarrow$ una valida funzione Hash è quella data dal metodo delle divisioni, ovvero:
							\begin{tcenter}
								$\bm{h(k)=k \mod m}$
							\end{tcenter} 
Un buon criterio di scelta per la variabile $m$ è un numero primo non troppo vicino ad una potenza del 2 (se si usa una potenza del 2 si rischia di raggruppare i valori sulla base dei loro valori meno significativi).
						\item[Risoluzione delle Collisioni] $\rightarrow$ le funzioni Hash non sono iniettive quindi dovrò risolvere le collisioni che si creeranno, esistono 2 metodi: 
						\begin{descr}{0}
							\item[Concatenamento] $\rightarrow$ tutti gli elementi con lo stesso Hash sono memorizzati in una lista concatenata. 
							\item[Indirizzamento Aperto] $\rightarrow$
						\end{descr}
					\end{descr}
			\end{descr}
		\end{myParagraph}


%TODO Aggiungere 2° parte tabelle hash










	\end{formulario}
\end{document}